{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "356b5692-bd13-423a-8351-81a41dbe1d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e3ac8b2-40a0-4632-bad7-1b9d7c440a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRec(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(AutoRec, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.act(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227de03f-bb65-47e5-8abf-7ecf202fcac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_AutorRec(outputs, targets, parameters, l2_weight):\n",
    "    mse_fn = nn.MSELoss()\n",
    "    mse = mse_fn(torch.where(targets != 0, outputs, 0.), targets)\n",
    "    l2 = 0\n",
    "    for param in parameters:\n",
    "        l2 += torch.norm(param)**2\n",
    "    return mse + l2_weight*l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9759e7-3008-493d-b6cb-1085105d5104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Reversal Layer\n",
    "class GRL(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(context, x, constant):\n",
    "        context.constant = constant\n",
    "        return x.view_as(x)*constant\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(context, grad):\n",
    "        return grad.neg()*context.constant, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "653a6078-0bc4-47c5-8dee-698b60f71ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DARec(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim, hidden_dim,\n",
    "                 RP_dim1, RP_dim2, output_dim,\n",
    "                 GRL_scale,\n",
    "                 DC_dim):\n",
    "        super(DARec, self).__init__()\n",
    "        # Rating Pattern Extractor\n",
    "        self.RPE = nn.Linear(input_dim, hidden_dim)\n",
    "        # Rating Predictor for Domain1\n",
    "        self.RP1_ln1 = nn.Linear(hidden_dim, RP_dim1)\n",
    "        self.RP1_ln2 = nn.Linear(RP_dim1, RP_dim2)\n",
    "        self.RP1_ln3 = nn.Linear(RP_dim2, output_dim)\n",
    "        # Rating Predictor for Domain2\n",
    "        self.RP2_ln1 = nn.Linear(hidden_dim, RP_dim1)\n",
    "        self.RP2_ln2 = nn.Linear(RP_dim1, RP_dim2)\n",
    "        self.RP2_ln3 = nn.Linear(RP_dim2, output_dim)\n",
    "        # Gradient Reverse Layer\n",
    "        self.GRL_scale = torch.tensor(GRL_scale)\n",
    "        # Domain Classifier\n",
    "        self.DC_ln1 = nn.Linear(hidden_dim, DC_dim)\n",
    "        self.DC_ln2 = nn.Linear(DC_dim, 2)\n",
    "        # activation function\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, flag):\n",
    "        x = self.RPE(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        x_hat1 = self.RP1_ln1(x)\n",
    "        x_hat1 = self.act(x_hat1)\n",
    "        x_hat1 = self.RP1_ln2(x_hat1)\n",
    "        x_hat1 = self.act(x_hat1)\n",
    "        x_hat1 = self.RP1_ln3(x_hat1)\n",
    "        \n",
    "        x_hat2 = self.RP2_ln1(x)\n",
    "        x_hat2 = self.act(x_hat2)\n",
    "        x_hat2 = self.RP2_ln2(x_hat2)\n",
    "        x_hat2 = self.act(x_hat2)\n",
    "        x_hat2 = self.RP2_ln3(x_hat2)\n",
    "        \n",
    "        x_class = GRL.apply(x, self.GRL_scale)\n",
    "        x_class = self.DC_ln1(x_class)\n",
    "        x_class = self.act(x_class)\n",
    "        x_class = self.DC_ln2(x_class)\n",
    "        \n",
    "        return x_hat1, x_hat2, x_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3243fe1-add1-482d-acbc-5a854b9c66cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_DARec(rating_pred1, rating_targets1,\n",
    "               rating_pred2, rating_targets2, RPloss_weight,\n",
    "               domain_pred, domain_targets, DCloss_weight,\n",
    "               parameters, l2_weight):\n",
    "    def loss_predictor(outputs, targets):\n",
    "        mse_fn = nn.MSELoss()\n",
    "        mse = mse_fn(torch.where(targets != 0, outputs, 0.), targets)\n",
    "        return mse\n",
    "    loss_pred1 = loss_predictor(rating_pred1, rating_targets1)\n",
    "    loss_pred2 = loss_predictor(rating_pred2, rating_targets2)\n",
    "    \n",
    "    crossentropy_fn = nn.CrossEntropyLoss()\n",
    "    loss_classisfy = crossentropy_fn(domain_pred, domain_targets)\n",
    "    \n",
    "    l2 = 0\n",
    "    for param in parameters:\n",
    "        l2 += torch.norm(param)**2\n",
    "    \n",
    "    return loss_pred1 + RPloss_weight*loss_pred2 - DCloss_weight*loss_classisfy + l2_weight*l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f284d6-4f3f-4c0a-b632-8aef44d13621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
